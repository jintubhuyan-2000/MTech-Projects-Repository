{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dde4c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting focal-lossNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading focal_loss-0.0.7-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: tensorflow>=2.2 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from focal-loss) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow>=2.2->focal-loss) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (4.24.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (68.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (1.57.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mrg22-020914479\\.conda\\envs\\testdl\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow>=2.2->focal-loss) (3.2.2)\n",
      "Installing collected packages: focal-loss\n",
      "Successfully installed focal-loss-0.0.7\n"
     ]
    }
   ],
   "source": [
    "pip install focal-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e3bb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from datetime import datetime \n",
    "import cv2\n",
    "from PIL import Image\n",
    "from keras import backend, optimizers\n",
    "from focal_loss import BinaryFocalLoss\n",
    "import glob\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from keras.utils import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65ad7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask=\"D:/Jintu/Jintu_LULC/TestDL_5/Data Preparation/Patchify/64_patches/lulc/\"\n",
    "train_ndvi=\"D:/Jintu/Jintu_LULC/TestDL_5/Data Preparation/Patchify/64_patches/ndvi/\"\n",
    "train_ndwi=\"D:/Jintu/Jintu_LULC/TestDL_5/Data Preparation/Patchify/64_patches/ndwi/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1600b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.3291682   0.27968127  0.3567867  ...  0.32540697  0.35557264\n",
      "    0.35557264]\n",
      "  [ 0.23272586  0.2771192   0.2735982  ...  0.35730118  0.3415681\n",
      "    0.33439717]\n",
      "  [ 0.23069175  0.23069175  0.2917826  ...  0.34138244  0.33800027\n",
      "    0.37672454]\n",
      "  ...\n",
      "  [ 0.56160396  0.54388714  0.5873753  ...  0.48962024  0.4733302\n",
      "    0.4269691 ]\n",
      "  [ 0.57008964  0.4806313   0.5158325  ...  0.46572718  0.43419653\n",
      "    0.4806196 ]\n",
      "  [ 0.54991084  0.47928995  0.4896868  ...  0.44864866  0.45850757\n",
      "    0.47351563]]\n",
      "\n",
      " [[ 0.324511    0.37125894  0.36278826 ...  0.22416994  0.23662326\n",
      "    0.2684797 ]\n",
      "  [ 0.33100942  0.3402877   0.36278826 ...  0.2748493   0.22477587\n",
      "    0.27924204]\n",
      "  [ 0.2951677   0.34788033  0.35102805 ...  0.2733478   0.26654866\n",
      "    0.24085073]\n",
      "  ...\n",
      "  [ 0.410156    0.4633826   0.44020358 ...  0.5152062   0.4372523\n",
      "    0.4372523 ]\n",
      "  [ 0.4513695   0.5215805   0.45291543 ...  0.41878876  0.46264857\n",
      "    0.41713902]\n",
      "  [ 0.4449294   0.4449294   0.48188654 ...  0.42289495  0.39005998\n",
      "    0.42327803]]\n",
      "\n",
      " [[ 0.4291322   0.43048912  0.38072526 ...  0.13825016  0.11885805\n",
      "    0.10234772]\n",
      "  [ 0.40003684  0.43056995  0.40282515 ...  0.10882526  0.09006798\n",
      "    0.07887108]\n",
      "  [ 0.41145834  0.4268465   0.3816174  ...  0.08491453  0.06932161\n",
      "    0.06562233]\n",
      "  ...\n",
      "  [ 0.35532424  0.35532424  0.4446964  ...  0.41771382  0.42665792\n",
      "    0.41034243]\n",
      "  [ 0.42094684  0.39803094  0.26661023 ...  0.40469906  0.41591758\n",
      "    0.4085928 ]\n",
      "  [ 0.3591178   0.33254835  0.45299718 ...  0.44846916  0.44303966\n",
      "    0.4085928 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.40222508  0.3291033   0.3343471  ...  0.37117684  0.4496407\n",
      "    0.42977226]\n",
      "  [ 0.3753267   0.41003445  0.35539335 ...  0.5238738   0.5398069\n",
      "    0.3899512 ]\n",
      "  [ 0.3699828   0.3767798   0.33083895 ...  0.34123352  0.50428027\n",
      "    0.47961956]\n",
      "  ...\n",
      "  [ 0.38450804  0.3489353   0.43698496 ...  0.45391476  0.54718596\n",
      "    0.45507243]\n",
      "  [ 0.39750412  0.40077567  0.37335944 ...  0.521328    0.52114403\n",
      "    0.38545653]\n",
      "  [ 0.297309    0.41245973  0.43625855 ...  0.47614053  0.4738399\n",
      "    0.4284895 ]]\n",
      "\n",
      " [[ 0.4294579   0.38032836  0.41217667 ...  0.4571401   0.5674326\n",
      "    0.48713446]\n",
      "  [ 0.46623254  0.416991    0.4513991  ...  0.45129016  0.55830437\n",
      "    0.51865256]\n",
      "  [ 0.4527001   0.4527001   0.46787897 ...  0.45467272  0.50715744\n",
      "    0.43801427]\n",
      "  ...\n",
      "  [ 0.5820477   0.6390565   0.6574027  ...  0.42845586  0.39465815\n",
      "    0.42471722]\n",
      "  [ 0.49541622  0.43884894  0.48004395 ...  0.396158    0.38343975\n",
      "    0.37082067]\n",
      "  [ 0.480934    0.48782292  0.61378825 ...  0.3556231   0.3400474\n",
      "    0.38917106]]\n",
      "\n",
      " [[ 0.5524976   0.48289558  0.48339042 ...  0.35541958  0.42457142\n",
      "    0.48832202]\n",
      "  [ 0.5381706   0.5178343   0.47340775 ...  0.48323813  0.47480294\n",
      "    0.44552645]\n",
      "  [ 0.48757857  0.40827116  0.47701815 ...  0.42323437  0.42700133\n",
      "    0.42741358]\n",
      "  ...\n",
      "  [ 0.30967647  0.10872632 -0.03919474 ...  0.38505945  0.39075464\n",
      "    0.40574506]\n",
      "  [ 0.13751519  0.01655051 -0.05676549 ...  0.38692853  0.3419386\n",
      "    0.37373102]\n",
      "  [ 0.38917106  0.28014234  0.06608856 ...  0.33858     0.36785153\n",
      "    0.3559852 ]]]\n",
      "[[[0.52363414 0.5093985  0.46296296 ... 0.7497898  0.7568741  0.7568741 ]\n",
      "  [0.42100656 0.42643017 0.42496848 ... 0.7547913  0.7435396  0.71630555]\n",
      "  [0.49014026 0.49014026 0.4937795  ... 0.75193655 0.7530109  0.7477583 ]\n",
      "  ...\n",
      "  [0.59880877 0.6131013  0.5910677  ... 0.79760176 0.7986026  0.7844619 ]\n",
      "  [0.615175   0.6053333  0.6409878  ... 0.79609036 0.7929047  0.79108024]\n",
      "  [0.5773242  0.59095895 0.6095749  ... 0.7885899  0.79957855 0.79806024]]\n",
      "\n",
      " [[0.7286552  0.7469079  0.7643573  ... 0.6763274  0.6605806  0.64078724]\n",
      "  [0.7389535  0.7594625  0.7643573  ... 0.621269   0.556338   0.56303936]\n",
      "  [0.74682593 0.76530135 0.75906545 ... 0.5796766  0.5489178  0.5113185 ]\n",
      "  ...\n",
      "  [0.79863644 0.80215156 0.81070536 ... 0.5065868  0.53628916 0.53628916]\n",
      "  [0.7945572  0.8005865  0.7960812  ... 0.58120674 0.5990719  0.64696765]\n",
      "  [0.7896494  0.7896494  0.7949722  ... 0.6099336  0.60518086 0.6741652 ]]\n",
      "\n",
      " [[0.7449469  0.74943566 0.72948605 ... 0.1695828  0.1340179  0.11210386]\n",
      "  [0.7305546  0.743606   0.7299029  ... 0.16152701 0.12881929 0.11121505]\n",
      "  [0.6961593  0.6815016  0.59714663 ... 0.1699552  0.13517472 0.12086183]\n",
      "  ...\n",
      "  [0.62083495 0.62083495 0.5133281  ... 0.7828842  0.78698224 0.7824174 ]\n",
      "  [0.6022259  0.600322   0.52899104 ... 0.77058977 0.78038114 0.7893496 ]\n",
      "  [0.6147829  0.60331917 0.6292829  ... 0.77411973 0.78089654 0.7893496 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.63055104 0.64401007 0.685879   ... 0.66295224 0.69108146 0.72989595]\n",
      "  [0.73320156 0.7192146  0.7200494  ... 0.6412866  0.72625417 0.7244164 ]\n",
      "  [0.763793   0.7622631  0.71352106 ... 0.6703002  0.74664813 0.7673192 ]\n",
      "  ...\n",
      "  [0.72872496 0.6833793  0.7003211  ... 0.53011733 0.58552635 0.4920292 ]\n",
      "  [0.7083567  0.676812   0.6912314  ... 0.58806753 0.5944326  0.47069064]\n",
      "  [0.69942194 0.6924067  0.6796952  ... 0.51588583 0.6001606  0.64488447]]\n",
      "\n",
      " [[0.7321685  0.7606539  0.7016469  ... 0.75326353 0.7348998  0.7230948 ]\n",
      "  [0.7426202  0.7249827  0.60733944 ... 0.7309383  0.70992833 0.70415497]\n",
      "  [0.7491353  0.7491353  0.6573544  ... 0.72448134 0.7156991  0.6853102 ]\n",
      "  ...\n",
      "  [0.33123526 0.2862678  0.31991953 ... 0.72584    0.71016693 0.7173233 ]\n",
      "  [0.4023384  0.42996424 0.38757294 ... 0.7477314  0.7174789  0.70114607]\n",
      "  [0.6238718  0.5249622  0.44050235 ... 0.73372203 0.73980886 0.72530353]]\n",
      "\n",
      " [[0.7088761  0.7073315  0.6708054  ... 0.76013887 0.79071164 0.81016165]\n",
      "  [0.65666664 0.6418678  0.6421515  ... 0.7284261  0.77092415 0.7756876 ]\n",
      "  [0.6790541  0.6520153  0.6155471  ... 0.74320936 0.7682691  0.7768546 ]\n",
      "  ...\n",
      "  [0.6752169  0.51183206 0.4040595  ... 0.7820694  0.7852485  0.7825163 ]\n",
      "  [0.61484545 0.38201916 0.34799197 ... 0.7953131  0.79361594 0.7906009 ]\n",
      "  [0.72530353 0.5665423  0.34672204 ... 0.7923767  0.7947043  0.7774438 ]]]\n",
      "(900, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "#Capture training image info as a list\n",
    "train_masks = []\n",
    "# Assuming train_image is the directory pattern you want to search\n",
    "for directory_path in glob.glob(train_mask):\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
    "        with rasterio.open(img_path) as src:\n",
    "            img = src.read(1)  # Read the image as a numpy array (assuming it's a single-band image)\n",
    "            train_masks.append(img)\n",
    "#Convert list to array for machine learning processing        \n",
    "train_masks = np.array(train_masks)\n",
    "\n",
    "#Capture training image info as a list\n",
    "train_ndvis = []\n",
    "# Assuming train_image is the directory pattern you want to search\n",
    "for directory_path in glob.glob(train_ndvi):\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
    "        with rasterio.open(img_path) as src:\n",
    "            img = src.read(1)  # Read the image as a numpy array (assuming it's a single-band image)\n",
    "            train_ndvis.append(img)\n",
    "            \n",
    "#Capture training image info as a list\n",
    "train_ndwis = []\n",
    "# Assuming train_image is the directory pattern you want to search\n",
    "for directory_path in glob.glob(train_ndwi):\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
    "        with rasterio.open(img_path) as src:\n",
    "            img = src.read(1)  # Read the image as a numpy array (assuming it's a single-band image)\n",
    "            train_ndwis.append(img)\n",
    "            \n",
    "#Convert list to array for machine learning processing        \n",
    "train_ndwis = np.array(train_ndwis)\n",
    "print(train_ndwis)\n",
    "            \n",
    "#Convert list to array for machine learning processing        \n",
    "train_ndvis = np.array(train_ndvis)\n",
    "print(train_ndvis)\n",
    "print(train_ndvis.shape)\n",
    "\n",
    "train_ndvis= np.expand_dims(train_ndvis, axis=3)\n",
    "train_ndvis = normalize(train_ndvis, axis=3)\n",
    "\n",
    "train_ndwis= np.expand_dims(train_ndwis, axis=3)\n",
    "train_ndwis = normalize(train_ndwis, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03f81ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrg22-020914479\\.conda\\envs\\TestDL\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w = train_masks.shape\n",
    "train_masks_reshaped = train_masks.reshape(-1,1)\n",
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "np.unique(train_masks_encoded_original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "625e49ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "print(train_masks_encoded_original_shape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06c22d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = np.expand_dims(train_masks_encoded_original_shape, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d40e867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a subset of data for quick testing\n",
    "#Picking 10% for testing and remaining for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1, X_test1,X2,X_test2,y1, y_test = train_test_split(train_ndvis,train_ndwis, train_masks, test_size = 0.10, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b6118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
    "\n",
    "\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "\n",
    "def jacard_coef_loss(y_true, y_pred):\n",
    "    return -jacard_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f91599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filter_size, size, dropout, batch_norm=False):\n",
    "    \n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(x)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "\n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "    \n",
    "    if dropout > 0:\n",
    "        conv = layers.Dropout(dropout)(conv)\n",
    "\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66579812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_elem(tensor, rep):\n",
    "    # lambda function to repeat Repeats the elements of a tensor along an axis\n",
    "    #by a factor of rep.\n",
    "    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape \n",
    "    #(None, 256,256,6), if specified axis=3 and rep=2.\n",
    "\n",
    "     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
    "                          arguments={'repnum': rep})(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad48138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_conv_block(x, filter_size, size, dropout, batch_norm=False):\n",
    "\n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    conv = layers.Activation('relu')(conv)\n",
    "    \n",
    "    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = layers.BatchNormalization(axis=3)(conv)\n",
    "    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut\n",
    "    if dropout > 0:\n",
    "        conv = layers.Dropout(dropout)(conv)\n",
    "\n",
    "    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        shortcut = layers.BatchNormalization(axis=3)(shortcut)\n",
    "\n",
    "    res_path = layers.add([shortcut, conv])\n",
    "    res_path = layers.Activation('relu')(res_path)    #Activation after addition with shortcut (Original residual block)\n",
    "    return res_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dca5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gating_signal(input, out_size, batch_norm=False):\n",
    "    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)\n",
    "    if batch_norm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5939ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(x, gating, inter_shape):\n",
    "    shape_x = K.int_shape(x)\n",
    "    shape_g = K.int_shape(gating)\n",
    "\n",
    "# Getting the x signal to the same shape as the gating signal\n",
    "    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n",
    "    shape_theta_x = K.int_shape(theta_x)\n",
    "\n",
    "# Getting the gating signal to the same number of filters as the inter_shape\n",
    "    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gating)\n",
    "    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),\n",
    "                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n",
    "                                 padding='same')(phi_g)  # 16\n",
    "\n",
    "    concat_xg = layers.add([upsample_g, theta_x])\n",
    "    act_xg = layers.Activation('relu')(concat_xg)\n",
    "    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)\n",
    "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
    "\n",
    "    upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n",
    "\n",
    "    y = layers.multiply([upsample_psi, x])\n",
    "\n",
    "    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)\n",
    "    result_bn = layers.BatchNormalization()(result)\n",
    "    return result_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39419c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):\n",
    "\n",
    "    # network structure\n",
    "    FILTER_NUM = 64 # number of filters for the first layer\n",
    "    FILTER_SIZE = 3 # size of the convolutional filter\n",
    "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
    "    \n",
    "\n",
    "    inputs = layers.Input(input_shape, dtype=tf.float32)\n",
    "\n",
    "    # Downsampling layers\n",
    "    # DownRes 1, convolution + pooling\n",
    "    conv_128 = conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
    "    # DownRes 2\n",
    "    conv_64 = conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
    "    # DownRes 3\n",
    "    conv_32 = conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
    "    # DownRes 4\n",
    "    conv_16 = conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
    "    # DownRes 5, convolution only\n",
    "    conv_8 = conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)\n",
    "\n",
    "    # Upsampling layers\n",
    "   \n",
    "    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n",
    "    up_16 = layers.concatenate([up_16, conv_16], axis=3)\n",
    "    up_conv_16 = conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 7\n",
    "    \n",
    "    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_16)\n",
    "    up_32 = layers.concatenate([up_32, conv_32], axis=3)\n",
    "    up_conv_32 = conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 8\n",
    "    \n",
    "    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_32)\n",
    "    up_64 = layers.concatenate([up_64, conv_64], axis=3)\n",
    "    up_conv_64 = conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 9\n",
    "   \n",
    "    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_64)\n",
    "    up_128 = layers.concatenate([up_128, conv_128], axis=3)\n",
    "    up_conv_128 = conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "\n",
    "    # 1*1 convolutional layers\n",
    "   \n",
    "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)\n",
    "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
    "    conv_final = layers.Activation('sigmoid')(conv_final)  #Change to softmax for multichannel\n",
    "\n",
    "    # Model \n",
    "    model = models.Model(inputs, conv_final, name=\"UNet\")\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93a0fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13 inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e942e497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"UNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_58 (InputLayer)       [(None, 64, 64, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " input_59 (InputLayer)       [(None, 64, 64, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d_247 (Conv2D)         (None, 64, 64, 64)           640       ['input_58[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_249 (Conv2D)         (None, 64, 64, 64)           640       ['input_59[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_247 (B  (None, 64, 64, 64)           256       ['conv2d_247[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_249 (B  (None, 64, 64, 64)           256       ['conv2d_249[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_248 (Conv2D)         (None, 64, 64, 64)           36928     ['batch_normalization_247[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_250 (Conv2D)         (None, 64, 64, 64)           36928     ['batch_normalization_249[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_248 (B  (None, 64, 64, 64)           256       ['conv2d_248[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_250 (B  (None, 64, 64, 64)           256       ['conv2d_250[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenat  (None, 64, 64, 128)          0         ['batch_normalization_248[0][0\n",
      " e)                                                                 ]',                           \n",
      "                                                                     'batch_normalization_250[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " up_sampling2d_52 (UpSampli  (None, 128, 128, 128)        0         ['concatenate_19[0][0]']      \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_251 (Conv2D)         (None, 128, 128, 512)        590336    ['up_sampling2d_52[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_251 (B  (None, 128, 128, 512)        2048      ['conv2d_251[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_252 (Conv2D)         (None, 128, 128, 512)        2359808   ['batch_normalization_251[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_252 (B  (None, 128, 128, 512)        2048      ['conv2d_252[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " up_sampling2d_53 (UpSampli  (None, 256, 256, 512)        0         ['batch_normalization_252[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " conv2d_253 (Conv2D)         (None, 256, 256, 256)        1179904   ['up_sampling2d_53[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_253 (B  (None, 256, 256, 256)        1024      ['conv2d_253[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_254 (Conv2D)         (None, 256, 256, 256)        590080    ['batch_normalization_253[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_254 (B  (None, 256, 256, 256)        1024      ['conv2d_254[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " up_sampling2d_54 (UpSampli  (None, 512, 512, 256)        0         ['batch_normalization_254[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " conv2d_255 (Conv2D)         (None, 512, 512, 128)        295040    ['up_sampling2d_54[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_255 (B  (None, 512, 512, 128)        512       ['conv2d_255[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_256 (Conv2D)         (None, 512, 512, 128)        147584    ['batch_normalization_255[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_256 (B  (None, 512, 512, 128)        512       ['conv2d_256[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " up_sampling2d_55 (UpSampli  (None, 1024, 1024, 128)      0         ['batch_normalization_256[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " conv2d_257 (Conv2D)         (None, 1024, 1024, 64)       73792     ['up_sampling2d_55[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_257 (B  (None, 1024, 1024, 64)       256       ['conv2d_257[0][0]']          \n",
      " atchNormalization)                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_258 (Conv2D)         (None, 1024, 1024, 64)       36928     ['batch_normalization_257[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_258 (B  (None, 1024, 1024, 64)       256       ['conv2d_258[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_259 (Conv2D)         (None, 1024, 1024, 6)        390       ['batch_normalization_258[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_259 (B  (None, 1024, 1024, 6)        24        ['conv2d_259[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_49 (Activation)  (None, 1024, 1024, 6)        0         ['batch_normalization_259[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5357726 (20.44 MB)\n",
      "Trainable params: 5353362 (20.42 MB)\n",
      "Non-trainable params: 4364 (17.05 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def UNet(input_shape, num_inputs=2, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):\n",
    "    # Network structure\n",
    "    FILTER_NUM = 64  # Number of filters for the first layer\n",
    "    FILTER_SIZE = 3  # Size of the convolutional filter\n",
    "    UP_SAMP_SIZE = 2  # Size of upsampling filters\n",
    "\n",
    "    input_layers = [layers.Input(input_shape, dtype=tf.float32) for _ in range(num_inputs)]\n",
    "\n",
    "    # Process each input separately\n",
    "    processed_inputs = []\n",
    "    for i, input_layer in enumerate(input_layers):\n",
    "        conv = conv_block(input_layer, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "        processed_inputs.append(conv)\n",
    "\n",
    "    # Merge the processed inputs\n",
    "    merged = layers.concatenate(processed_inputs, axis=3)\n",
    "\n",
    "    # Upsampling layers\n",
    "    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE))(merged)\n",
    "    up_conv_16 = conv_block(up_16, FILTER_SIZE, 8 * FILTER_NUM, dropout_rate, batch_norm)\n",
    "    \n",
    "    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE))(up_conv_16)\n",
    "    up_conv_32 = conv_block(up_32, FILTER_SIZE, 4 * FILTER_NUM, dropout_rate, batch_norm)\n",
    "    \n",
    "    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE))(up_conv_32)\n",
    "    up_conv_64 = conv_block(up_64, FILTER_SIZE, 2 * FILTER_NUM, dropout_rate, batch_norm)\n",
    "    \n",
    "    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE))(up_conv_64)\n",
    "    up_conv_128 = conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "\n",
    "    # 1*1 convolutional layers\n",
    "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1, 1))(up_conv_128)\n",
    "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
    "    conv_final = layers.Activation('sigmoid')(conv_final)  # Change to softmax for multichannel\n",
    "\n",
    "    # Model\n",
    "    model = models.Model(inputs=input_layers, outputs=conv_final, name=\"UNet\")\n",
    "    return model\n",
    "\n",
    "def conv_block(input_layer, filter_size, num_filters, dropout_rate, batch_norm):\n",
    "    # Define a convolutional block with optional dropout and batch normalization\n",
    "    x = layers.Conv2D(num_filters, (filter_size, filter_size), activation='relu', padding='same')(input_layer)\n",
    "    if batch_norm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    if dropout_rate > 0.0:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Conv2D(num_filters, (filter_size, filter_size), activation='relu', padding='same')(x)\n",
    "    if batch_norm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (64, 64, 1)  # Adjust input shape as needed\n",
    "num_inputs = 2  # Number of input tensors\n",
    "model = UNet(input_shape, num_inputs=num_inputs, NUM_CLASSES=6, dropout_rate=0.0, batch_norm=True)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb768bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e0ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bd6c5ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Attention_UNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_60 (InputLayer)       [(None, 64, 64, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d_260 (Conv2D)         (None, 64, 64, 64)           640       ['input_60[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_260 (B  (None, 64, 64, 64)           256       ['conv2d_260[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_261 (Conv2D)         (None, 64, 64, 64)           36928     ['batch_normalization_260[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_261 (B  (None, 64, 64, 64)           256       ['conv2d_261[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPoolin  (None, 32, 32, 64)           0         ['batch_normalization_261[0][0\n",
      " g2D)                                                               ]']                           \n",
      "                                                                                                  \n",
      " conv2d_262 (Conv2D)         (None, 32, 32, 128)          73856     ['max_pooling2d_8[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_262 (B  (None, 32, 32, 128)          512       ['conv2d_262[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_263 (Conv2D)         (None, 32, 32, 128)          147584    ['batch_normalization_262[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_263 (B  (None, 32, 32, 128)          512       ['conv2d_263[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPoolin  (None, 16, 16, 128)          0         ['batch_normalization_263[0][0\n",
      " g2D)                                                               ]']                           \n",
      "                                                                                                  \n",
      " conv2d_264 (Conv2D)         (None, 16, 16, 256)          295168    ['max_pooling2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_264 (B  (None, 16, 16, 256)          1024      ['conv2d_264[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_265 (Conv2D)         (None, 16, 16, 256)          590080    ['batch_normalization_264[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_265 (B  (None, 16, 16, 256)          1024      ['conv2d_265[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooli  (None, 8, 8, 256)            0         ['batch_normalization_265[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " conv2d_266 (Conv2D)         (None, 8, 8, 512)            1180160   ['max_pooling2d_10[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_266 (B  (None, 8, 8, 512)            2048      ['conv2d_266[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_267 (Conv2D)         (None, 8, 8, 512)            2359808   ['batch_normalization_266[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_267 (B  (None, 8, 8, 512)            2048      ['conv2d_267[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooli  (None, 4, 4, 512)            0         ['batch_normalization_267[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " conv2d_268 (Conv2D)         (None, 4, 4, 1024)           4719616   ['max_pooling2d_11[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_268 (B  (None, 4, 4, 1024)           4096      ['conv2d_268[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_269 (Conv2D)         (None, 4, 4, 1024)           9438208   ['batch_normalization_268[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_269 (B  (None, 4, 4, 1024)           4096      ['conv2d_269[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_270 (Conv2D)         (None, 4, 4, 512)            524800    ['batch_normalization_269[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_270 (B  (None, 4, 4, 512)            2048      ['conv2d_270[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_50 (Activation)  (None, 4, 4, 512)            0         ['batch_normalization_270[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_272 (Conv2D)         (None, 4, 4, 512)            262656    ['activation_50[0][0]']       \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_transpose (Conv2DTr  (None, 4, 4, 512)            2359808   ['conv2d_272[0][0]']          \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_271 (Conv2D)         (None, 4, 4, 512)            1049088   ['batch_normalization_267[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 4, 4, 512)            0         ['conv2d_transpose[0][0]',    \n",
      "                                                                     'conv2d_271[0][0]']          \n",
      "                                                                                                  \n",
      " activation_51 (Activation)  (None, 4, 4, 512)            0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_273 (Conv2D)         (None, 4, 4, 1)              513       ['activation_51[0][0]']       \n",
      "                                                                                                  \n",
      " activation_52 (Activation)  (None, 4, 4, 1)              0         ['conv2d_273[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_56 (UpSampli  (None, 8, 8, 1)              0         ['activation_52[0][0]']       \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 8, 8, 512)            0         ['up_sampling2d_56[0][0]']    \n",
      "                                                                                                  \n",
      " multiply (Multiply)         (None, 8, 8, 512)            0         ['lambda[0][0]',              \n",
      "                                                                     'batch_normalization_267[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_274 (Conv2D)         (None, 8, 8, 512)            262656    ['multiply[0][0]']            \n",
      "                                                                                                  \n",
      " up_sampling2d_57 (UpSampli  (None, 8, 8, 1024)           0         ['batch_normalization_269[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_271 (B  (None, 8, 8, 512)            2048      ['conv2d_274[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenat  (None, 8, 8, 1536)           0         ['up_sampling2d_57[0][0]',    \n",
      " e)                                                                  'batch_normalization_271[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_275 (Conv2D)         (None, 8, 8, 512)            7078400   ['concatenate_20[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_272 (B  (None, 8, 8, 512)            2048      ['conv2d_275[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_276 (Conv2D)         (None, 8, 8, 512)            2359808   ['batch_normalization_272[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_273 (B  (None, 8, 8, 512)            2048      ['conv2d_276[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_277 (Conv2D)         (None, 8, 8, 256)            131328    ['batch_normalization_273[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_274 (B  (None, 8, 8, 256)            1024      ['conv2d_277[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_53 (Activation)  (None, 8, 8, 256)            0         ['batch_normalization_274[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_279 (Conv2D)         (None, 8, 8, 256)            65792     ['activation_53[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, 8, 8, 256)            590080    ['conv2d_279[0][0]']          \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_278 (Conv2D)         (None, 8, 8, 256)            262400    ['batch_normalization_265[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 8, 8, 256)            0         ['conv2d_transpose_1[0][0]',  \n",
      "                                                                     'conv2d_278[0][0]']          \n",
      "                                                                                                  \n",
      " activation_54 (Activation)  (None, 8, 8, 256)            0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_280 (Conv2D)         (None, 8, 8, 1)              257       ['activation_54[0][0]']       \n",
      "                                                                                                  \n",
      " activation_55 (Activation)  (None, 8, 8, 1)              0         ['conv2d_280[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_58 (UpSampli  (None, 16, 16, 1)            0         ['activation_55[0][0]']       \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)           (None, 16, 16, 256)          0         ['up_sampling2d_58[0][0]']    \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)       (None, 16, 16, 256)          0         ['lambda_1[0][0]',            \n",
      "                                                                     'batch_normalization_265[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_281 (Conv2D)         (None, 16, 16, 256)          65792     ['multiply_1[0][0]']          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " up_sampling2d_59 (UpSampli  (None, 16, 16, 512)          0         ['batch_normalization_273[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_275 (B  (None, 16, 16, 256)          1024      ['conv2d_281[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenat  (None, 16, 16, 768)          0         ['up_sampling2d_59[0][0]',    \n",
      " e)                                                                  'batch_normalization_275[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_282 (Conv2D)         (None, 16, 16, 256)          1769728   ['concatenate_21[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_276 (B  (None, 16, 16, 256)          1024      ['conv2d_282[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_283 (Conv2D)         (None, 16, 16, 256)          590080    ['batch_normalization_276[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_277 (B  (None, 16, 16, 256)          1024      ['conv2d_283[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_284 (Conv2D)         (None, 16, 16, 128)          32896     ['batch_normalization_277[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_278 (B  (None, 16, 16, 128)          512       ['conv2d_284[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_56 (Activation)  (None, 16, 16, 128)          0         ['batch_normalization_278[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_286 (Conv2D)         (None, 16, 16, 128)          16512     ['activation_56[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, 16, 16, 128)          147584    ['conv2d_286[0][0]']          \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_285 (Conv2D)         (None, 16, 16, 128)          65664     ['batch_normalization_263[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 16, 16, 128)          0         ['conv2d_transpose_2[0][0]',  \n",
      "                                                                     'conv2d_285[0][0]']          \n",
      "                                                                                                  \n",
      " activation_57 (Activation)  (None, 16, 16, 128)          0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_287 (Conv2D)         (None, 16, 16, 1)            129       ['activation_57[0][0]']       \n",
      "                                                                                                  \n",
      " activation_58 (Activation)  (None, 16, 16, 1)            0         ['conv2d_287[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_60 (UpSampli  (None, 32, 32, 1)            0         ['activation_58[0][0]']       \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)           (None, 32, 32, 128)          0         ['up_sampling2d_60[0][0]']    \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)       (None, 32, 32, 128)          0         ['lambda_2[0][0]',            \n",
      "                                                                     'batch_normalization_263[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_288 (Conv2D)         (None, 32, 32, 128)          16512     ['multiply_2[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_61 (UpSampli  (None, 32, 32, 256)          0         ['batch_normalization_277[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_279 (B  (None, 32, 32, 128)          512       ['conv2d_288[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenat  (None, 32, 32, 384)          0         ['up_sampling2d_61[0][0]',    \n",
      " e)                                                                  'batch_normalization_279[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_289 (Conv2D)         (None, 32, 32, 128)          442496    ['concatenate_22[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_280 (B  (None, 32, 32, 128)          512       ['conv2d_289[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_290 (Conv2D)         (None, 32, 32, 128)          147584    ['batch_normalization_280[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_281 (B  (None, 32, 32, 128)          512       ['conv2d_290[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_291 (Conv2D)         (None, 32, 32, 64)           8256      ['batch_normalization_281[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_282 (B  (None, 32, 32, 64)           256       ['conv2d_291[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_59 (Activation)  (None, 32, 32, 64)           0         ['batch_normalization_282[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_293 (Conv2D)         (None, 32, 32, 64)           4160      ['activation_59[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, 32, 32, 64)           36928     ['conv2d_293[0][0]']          \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_292 (Conv2D)         (None, 32, 32, 64)           16448     ['batch_normalization_261[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 32, 32, 64)           0         ['conv2d_transpose_3[0][0]',  \n",
      "                                                                     'conv2d_292[0][0]']          \n",
      "                                                                                                  \n",
      " activation_60 (Activation)  (None, 32, 32, 64)           0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_294 (Conv2D)         (None, 32, 32, 1)            65        ['activation_60[0][0]']       \n",
      "                                                                                                  \n",
      " activation_61 (Activation)  (None, 32, 32, 1)            0         ['conv2d_294[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_62 (UpSampli  (None, 64, 64, 1)            0         ['activation_61[0][0]']       \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)           (None, 64, 64, 64)           0         ['up_sampling2d_62[0][0]']    \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)       (None, 64, 64, 64)           0         ['lambda_3[0][0]',            \n",
      "                                                                     'batch_normalization_261[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_295 (Conv2D)         (None, 64, 64, 64)           4160      ['multiply_3[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_63 (UpSampli  (None, 64, 64, 128)          0         ['batch_normalization_281[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_283 (B  (None, 64, 64, 64)           256       ['conv2d_295[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenat  (None, 64, 64, 192)          0         ['up_sampling2d_63[0][0]',    \n",
      " e)                                                                  'batch_normalization_283[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_296 (Conv2D)         (None, 64, 64, 64)           110656    ['concatenate_23[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_284 (B  (None, 64, 64, 64)           256       ['conv2d_296[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_297 (Conv2D)         (None, 64, 64, 64)           36928     ['batch_normalization_284[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_285 (B  (None, 64, 64, 64)           256       ['conv2d_297[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_298 (Conv2D)         (None, 64, 64, 1)            65        ['batch_normalization_285[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_286 (B  (None, 64, 64, 1)            4         ['conv2d_298[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_62 (Activation)  (None, 64, 64, 1)            0         ['batch_normalization_286[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 37333513 (142.42 MB)\n",
      "Trainable params: 37317895 (142.36 MB)\n",
      "Non-trainable params: 15618 (61.01 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def Attention_UNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):\n",
    "    # network structure\n",
    "    FILTER_NUM = 64 # number of basic filters for the first layer\n",
    "    FILTER_SIZE = 3 # size of the convolutional filter\n",
    "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
    "    \n",
    "    inputs = layers.Input(input_shape, dtype=tf.float32)\n",
    "\n",
    "    # Downsampling layers\n",
    "    # DownRes 1, convolution + pooling\n",
    "    conv_128 = conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
    "    # DownRes 2\n",
    "    conv_64 = conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
    "    # DownRes 3\n",
    "    conv_32 = conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
    "    # DownRes 4\n",
    "    conv_16 = conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
    "    # DownRes 5, convolution only\n",
    "    conv_8 = conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)\n",
    "\n",
    "    # Upsampling layers\n",
    "    # UpRes 6, attention gated concatenation + upsampling + double residual convolution\n",
    "    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)\n",
    "    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)\n",
    "    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n",
    "    up_16 = layers.concatenate([up_16, att_16], axis=3)\n",
    "    up_conv_16 = conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 7\n",
    "    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)\n",
    "    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)\n",
    "    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_16)\n",
    "    up_32 = layers.concatenate([up_32, att_32], axis=3)\n",
    "    up_conv_32 = conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 8\n",
    "    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)\n",
    "    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)\n",
    "    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_32)\n",
    "    up_64 = layers.concatenate([up_64, att_64], axis=3)\n",
    "    up_conv_64 = conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 9\n",
    "    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)\n",
    "    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)\n",
    "    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_64)\n",
    "    up_128 = layers.concatenate([up_128, att_128], axis=3)\n",
    "    up_conv_128 = conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "\n",
    "    # 1*1 convolutional layers\n",
    "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)\n",
    "    conv_final = layers.BatchNormalization(axis=3)(conv_final)\n",
    "    conv_final = layers.Activation('sigmoid')(conv_final)  #Change to softmax for multichannel\n",
    "\n",
    "    # Model integration\n",
    "    model = models.Model(inputs, conv_final, name=\"Attention_UNet\")\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (64, 64, 1)  # Adjust input shape as needed\n",
    "num_inputs = 2  # Number of input tensors\n",
    "model = Attention_UNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b36da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):\n",
    "\n",
    "    # network structure\n",
    "    FILTER_NUM = 64 # number of basic filters for the first layer\n",
    "    FILTER_SIZE = 3 # size of the convolutional filter\n",
    "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
    "    # input data\n",
    "    # dimension of the image depth\n",
    "    inputs = layers.Input(input_shape, dtype=tf.float32)\n",
    "    axis = 3\n",
    "\n",
    "    # Downsampling layers\n",
    "    # DownRes 1, double residual convolution + pooling\n",
    "    conv_128 = res_conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n",
    "    # DownRes 2\n",
    "    conv_64 = res_conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n",
    "    # DownRes 3\n",
    "    conv_32 = res_conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n",
    "    # DownRes 4\n",
    "    conv_16 = res_conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n",
    "    # DownRes 5, convolution only\n",
    "    conv_8 = res_conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)\n",
    "\n",
    "    # Upsampling layers\n",
    "    # UpRes 6, attention gated concatenation + upsampling + double residual convolution\n",
    "    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)\n",
    "    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)\n",
    "    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n",
    "    up_16 = layers.concatenate([up_16, att_16], axis=axis)\n",
    "    up_conv_16 = res_conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 7\n",
    "    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)\n",
    "    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)\n",
    "    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_16)\n",
    "    up_32 = layers.concatenate([up_32, att_32], axis=axis)\n",
    "    up_conv_32 = res_conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 8\n",
    "    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)\n",
    "    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)\n",
    "    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_32)\n",
    "    up_64 = layers.concatenate([up_64, att_64], axis=axis)\n",
    "    up_conv_64 = res_conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
    "    # UpRes 9\n",
    "    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)\n",
    "    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)\n",
    "    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_64)\n",
    "    up_128 = layers.concatenate([up_128, att_128], axis=axis)\n",
    "    up_conv_128 = res_conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
    "\n",
    "    # 1*1 convolutional layers\n",
    "    \n",
    "    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)\n",
    "    conv_final = layers.BatchNormalization(axis=axis)(conv_final)\n",
    "    conv_final = layers.Activation('sigmoid')(conv_final)  #Change to softmax for multichannel\n",
    "\n",
    "    # Model integration\n",
    "    model = models.Model(inputs, conv_final, name=\"AttentionResUNet\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8901aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a91c2653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(lr = 1e-2), loss=BinaryFocalLoss(gamma=2), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8feea4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4cf7af05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "102/102 [==============================] - 235s 2s/step - loss: 0.1848 - accuracy: 0.3005 - val_loss: 0.4335 - val_accuracy: 0.2579\n",
      "Epoch 2/50\n",
      "102/102 [==============================] - 212s 2s/step - loss: 0.1634 - accuracy: 0.2979 - val_loss: 0.2653 - val_accuracy: 0.2676\n",
      "Epoch 3/50\n",
      "102/102 [==============================] - 208s 2s/step - loss: 0.1534 - accuracy: 0.2832 - val_loss: 0.1809 - val_accuracy: 0.2673\n",
      "Epoch 4/50\n",
      "102/102 [==============================] - 229s 2s/step - loss: 0.1479 - accuracy: 0.2720 - val_loss: 0.1664 - val_accuracy: 0.2787\n",
      "Epoch 5/50\n",
      "102/102 [==============================] - 242s 2s/step - loss: 0.1436 - accuracy: 0.2684 - val_loss: 0.1659 - val_accuracy: 0.2796\n",
      "Epoch 6/50\n",
      "102/102 [==============================] - 241s 2s/step - loss: 0.1415 - accuracy: 0.2669 - val_loss: 0.2340 - val_accuracy: 0.2558\n",
      "Epoch 7/50\n",
      "102/102 [==============================] - 234s 2s/step - loss: 0.1438 - accuracy: 0.2644 - val_loss: 0.1383 - val_accuracy: 0.2963\n",
      "Epoch 8/50\n",
      "102/102 [==============================] - 235s 2s/step - loss: 0.1414 - accuracy: 0.2597 - val_loss: 0.1393 - val_accuracy: 0.2655\n",
      "Epoch 9/50\n",
      "102/102 [==============================] - 246s 2s/step - loss: 0.1421 - accuracy: 0.2677 - val_loss: 0.1369 - val_accuracy: 0.2558\n",
      "Epoch 10/50\n",
      "102/102 [==============================] - 250s 2s/step - loss: 0.1430 - accuracy: 0.2599 - val_loss: 0.1942 - val_accuracy: 0.2558\n",
      "Epoch 11/50\n",
      "102/102 [==============================] - 247s 2s/step - loss: 0.1409 - accuracy: 0.2601 - val_loss: 0.1340 - val_accuracy: 0.2558\n",
      "Epoch 12/50\n",
      "102/102 [==============================] - 244s 2s/step - loss: 0.1416 - accuracy: 0.2640 - val_loss: 0.1335 - val_accuracy: 0.2558\n",
      "Epoch 13/50\n",
      "102/102 [==============================] - 239s 2s/step - loss: 0.1389 - accuracy: 0.2603 - val_loss: 0.1339 - val_accuracy: 0.2558\n",
      "Epoch 14/50\n",
      "102/102 [==============================] - 233s 2s/step - loss: 0.1370 - accuracy: 0.2615 - val_loss: 0.1736 - val_accuracy: 0.2558\n",
      "Epoch 15/50\n",
      "102/102 [==============================] - 232s 2s/step - loss: 0.1358 - accuracy: 0.2606 - val_loss: 0.1547 - val_accuracy: 0.2558\n",
      "Epoch 16/50\n",
      "102/102 [==============================] - 229s 2s/step - loss: 0.1382 - accuracy: 0.2604 - val_loss: 0.2093 - val_accuracy: 0.2558\n",
      "Epoch 17/50\n",
      "102/102 [==============================] - 221s 2s/step - loss: 0.1381 - accuracy: 0.2572 - val_loss: 0.1441 - val_accuracy: 0.2558\n",
      "Epoch 18/50\n",
      "102/102 [==============================] - 226s 2s/step - loss: 0.1349 - accuracy: 0.2619 - val_loss: 0.1365 - val_accuracy: 0.2558\n",
      "Epoch 19/50\n",
      "102/102 [==============================] - 220s 2s/step - loss: 0.1357 - accuracy: 0.2584 - val_loss: 0.2921 - val_accuracy: 0.2558\n",
      "Epoch 20/50\n",
      "102/102 [==============================] - 260s 3s/step - loss: 0.1353 - accuracy: 0.2602 - val_loss: 0.2145 - val_accuracy: 0.2558\n",
      "Epoch 21/50\n",
      "102/102 [==============================] - 209s 2s/step - loss: 0.1330 - accuracy: 0.2616 - val_loss: 0.1849 - val_accuracy: 0.2558\n",
      "Epoch 22/50\n",
      "102/102 [==============================] - 211s 2s/step - loss: 0.1328 - accuracy: 0.2612 - val_loss: 0.1323 - val_accuracy: 0.2558\n",
      "Epoch 23/50\n",
      "102/102 [==============================] - 209s 2s/step - loss: 0.1318 - accuracy: 0.2624 - val_loss: 0.1607 - val_accuracy: 0.2558\n",
      "Epoch 24/50\n",
      "102/102 [==============================] - 211s 2s/step - loss: 0.1318 - accuracy: 0.2635 - val_loss: 0.1836 - val_accuracy: 0.2558\n",
      "Epoch 25/50\n",
      "102/102 [==============================] - 209s 2s/step - loss: 0.1328 - accuracy: 0.2616 - val_loss: 0.1718 - val_accuracy: 0.2558\n",
      "Epoch 26/50\n",
      "102/102 [==============================] - 208s 2s/step - loss: 0.1366 - accuracy: 0.2617 - val_loss: 0.3787 - val_accuracy: 0.2558\n",
      "Epoch 27/50\n",
      "102/102 [==============================] - 210s 2s/step - loss: 0.1396 - accuracy: 0.2581 - val_loss: 0.1326 - val_accuracy: 0.2558\n",
      "Epoch 28/50\n",
      "102/102 [==============================] - 211s 2s/step - loss: 0.1360 - accuracy: 0.2588 - val_loss: 0.2399 - val_accuracy: 0.2558\n",
      "Epoch 29/50\n",
      "102/102 [==============================] - 215s 2s/step - loss: 0.1349 - accuracy: 0.2640 - val_loss: 0.1647 - val_accuracy: 0.2558\n",
      "Epoch 30/50\n",
      "102/102 [==============================] - 211s 2s/step - loss: 0.1364 - accuracy: 0.2602 - val_loss: 0.1759 - val_accuracy: 0.2558\n",
      "Epoch 31/50\n",
      "102/102 [==============================] - 214s 2s/step - loss: 0.1337 - accuracy: 0.2598 - val_loss: 0.2414 - val_accuracy: 0.2558\n",
      "Epoch 32/50\n",
      "102/102 [==============================] - 210s 2s/step - loss: 0.1330 - accuracy: 0.2595 - val_loss: 0.2183 - val_accuracy: 0.2558\n",
      "Epoch 33/50\n",
      "102/102 [==============================] - 208s 2s/step - loss: 0.1330 - accuracy: 0.2679 - val_loss: 0.1653 - val_accuracy: 0.2558\n",
      "Epoch 34/50\n",
      "102/102 [==============================] - 211s 2s/step - loss: 0.1340 - accuracy: 0.2655 - val_loss: 0.3206 - val_accuracy: 0.2558\n",
      "Epoch 35/50\n",
      "102/102 [==============================] - 207s 2s/step - loss: 0.1319 - accuracy: 0.2660 - val_loss: 0.1638 - val_accuracy: 0.2558\n",
      "Epoch 36/50\n",
      "102/102 [==============================] - 206s 2s/step - loss: 0.1324 - accuracy: 0.2596 - val_loss: 0.2341 - val_accuracy: 0.2558\n",
      "Epoch 37/50\n",
      "102/102 [==============================] - 207s 2s/step - loss: 0.1314 - accuracy: 0.2621 - val_loss: 0.1807 - val_accuracy: 0.2556\n",
      "Epoch 38/50\n",
      "102/102 [==============================] - 208s 2s/step - loss: 0.1296 - accuracy: 0.2647 - val_loss: 0.2159 - val_accuracy: 0.2540\n",
      "Epoch 39/50\n",
      "102/102 [==============================] - 209s 2s/step - loss: 0.1296 - accuracy: 0.2678 - val_loss: 0.2085 - val_accuracy: 0.2543\n",
      "Epoch 40/50\n",
      "102/102 [==============================] - 207s 2s/step - loss: 0.1299 - accuracy: 0.2702 - val_loss: 0.1895 - val_accuracy: 0.2558\n",
      "Epoch 41/50\n",
      "102/102 [==============================] - 206s 2s/step - loss: 0.1320 - accuracy: 0.2622 - val_loss: 0.3159 - val_accuracy: 0.2558\n",
      "Epoch 42/50\n",
      "102/102 [==============================] - 207s 2s/step - loss: 0.1302 - accuracy: 0.2687 - val_loss: 0.1440 - val_accuracy: 0.2557\n",
      "Epoch 43/50\n",
      "102/102 [==============================] - 208s 2s/step - loss: 0.1288 - accuracy: 0.2699 - val_loss: 0.1371 - val_accuracy: 0.2558\n",
      "Epoch 44/50\n",
      "102/102 [==============================] - 208s 2s/step - loss: 0.1283 - accuracy: 0.2742 - val_loss: 0.2346 - val_accuracy: 0.2558\n",
      "Epoch 45/50\n",
      "102/102 [==============================] - 206s 2s/step - loss: 0.1294 - accuracy: 0.2693 - val_loss: 0.1857 - val_accuracy: 0.2558\n",
      "Epoch 46/50\n",
      "102/102 [==============================] - 205s 2s/step - loss: 0.1292 - accuracy: 0.2717 - val_loss: 0.1774 - val_accuracy: 0.2558\n",
      "Epoch 47/50\n",
      "102/102 [==============================] - 206s 2s/step - loss: 0.1291 - accuracy: 0.2719 - val_loss: 0.1410 - val_accuracy: 0.2533\n",
      "Epoch 48/50\n",
      "102/102 [==============================] - 207s 2s/step - loss: 0.1288 - accuracy: 0.2751 - val_loss: 0.1743 - val_accuracy: 0.2558\n",
      "Epoch 49/50\n",
      "102/102 [==============================] - 208s 2s/step - loss: 0.1273 - accuracy: 0.2768 - val_loss: 0.1863 - val_accuracy: 0.2558\n",
      "Epoch 50/50\n",
      "102/102 [==============================] - 206s 2s/step - loss: 0.1252 - accuracy: 0.2806 - val_loss: 0.1903 - val_accuracy: 0.2558\n"
     ]
    }
   ],
   "source": [
    "unet_history = model.fit(X1, y1, \n",
    "                    verbose=1,\n",
    "                    batch_size = 8,\n",
    "                    validation_data=(X_test1, y_test ), \n",
    "                    shuffle=False,\n",
    "                    epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266e5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385009d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6f2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143567f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514328e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9ee0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb242e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f39c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f31f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcf9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
